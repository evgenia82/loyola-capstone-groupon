{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.dates import MonthLocator, DateFormatter\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('groupon.txt', sep=None, engine='python')\n",
    "#df.to_parquet('groupon.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('groupon.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refund_bucket</th>\n",
       "      <th>refund_sub_bucket</th>\n",
       "      <th>order_date</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>week_end_date</th>\n",
       "      <th>dmm_subcat_1</th>\n",
       "      <th>category_1</th>\n",
       "      <th>deal_supply_channel</th>\n",
       "      <th>buyer_name_1</th>\n",
       "      <th>auth_bookings</th>\n",
       "      <th>capture_bookings</th>\n",
       "      <th>refunds</th>\n",
       "      <th>cancel_refunds</th>\n",
       "      <th>refunded_units</th>\n",
       "      <th>auth_refunds</th>\n",
       "      <th>capture_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>8/4/2016</td>\n",
       "      <td>8/4/2016</td>\n",
       "      <td>8/7/2016</td>\n",
       "      <td>Inverse Normal</td>\n",
       "      <td>Probability distribution II</td>\n",
       "      <td>Goods Stores</td>\n",
       "      <td>Asher</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>91.87</td>\n",
       "      <td>?</td>\n",
       "      <td>3</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Returns</td>\n",
       "      <td>Change of mind</td>\n",
       "      <td>8/31/2018</td>\n",
       "      <td>9/21/2018</td>\n",
       "      <td>9/23/2018</td>\n",
       "      <td>Binomial Distribution.</td>\n",
       "      <td>Probability distribution I</td>\n",
       "      <td>Goods</td>\n",
       "      <td>Jesus</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>20.98</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fraud</td>\n",
       "      <td>Fraud</td>\n",
       "      <td>4/19/2017</td>\n",
       "      <td>4/19/2017</td>\n",
       "      <td>4/23/2017</td>\n",
       "      <td>Power series</td>\n",
       "      <td>Calculus II</td>\n",
       "      <td>Goods</td>\n",
       "      <td>Tristan</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>79.94</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two-Hour Refunds</td>\n",
       "      <td>Two-Hour Refunds</td>\n",
       "      <td>2/5/2016</td>\n",
       "      <td>2/5/2016</td>\n",
       "      <td>2/7/2016</td>\n",
       "      <td>Prime Factorization Algorithms</td>\n",
       "      <td>?</td>\n",
       "      <td>Goods</td>\n",
       "      <td>Jeremiah</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>49.267469958</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shortage Cancellations</td>\n",
       "      <td>Vendor Shortage</td>\n",
       "      <td>7/21/2018</td>\n",
       "      <td>8/15/2018</td>\n",
       "      <td>8/19/2018</td>\n",
       "      <td>Transformations</td>\n",
       "      <td>Geometry</td>\n",
       "      <td>Goods</td>\n",
       "      <td>Jacob</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>29.97</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            refund_bucket refund_sub_bucket order_date transaction_date  \\\n",
       "0                   Other             Other   8/4/2016         8/4/2016   \n",
       "1                 Returns    Change of mind  8/31/2018        9/21/2018   \n",
       "2                   Fraud             Fraud  4/19/2017        4/19/2017   \n",
       "3        Two-Hour Refunds  Two-Hour Refunds   2/5/2016         2/5/2016   \n",
       "4  Shortage Cancellations   Vendor Shortage  7/21/2018        8/15/2018   \n",
       "\n",
       "  week_end_date                    dmm_subcat_1                   category_1  \\\n",
       "0      8/7/2016                  Inverse Normal  Probability distribution II   \n",
       "1     9/23/2018          Binomial Distribution.   Probability distribution I   \n",
       "2     4/23/2017                    Power series                  Calculus II   \n",
       "3      2/7/2016  Prime Factorization Algorithms                            ?   \n",
       "4     8/19/2018                 Transformations                     Geometry   \n",
       "\n",
       "  deal_supply_channel buyer_name_1 auth_bookings capture_bookings  \\\n",
       "0        Goods Stores       Asher              ?                ?   \n",
       "1               Goods       Jesus              ?                ?   \n",
       "2               Goods     Tristan              ?                ?   \n",
       "3               Goods    Jeremiah              ?                ?   \n",
       "4               Goods       Jacob              ?                ?   \n",
       "\n",
       "        refunds cancel_refunds refunded_units auth_refunds capture_units  \n",
       "0         91.87              ?              3            ?             ?  \n",
       "1         20.98              ?              1            ?             ?  \n",
       "2             ?          79.94              ?            ?             ?  \n",
       "3  49.267469958              ?              1            ?             ?  \n",
       "4         29.97              ?              2            ?             ?  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe consists of 16 columns and approximately 6.8 millions rows. The dataframe is huge as it occupies almost 1 GB of memory. Python loaded all columns as objects - so we need to transform data into right data type. The original dictionary with data types description is provided below.\n",
    "\n",
    "|Column_name\t                |Type\t\t|Description\n",
    "| --- | --- | --- |\n",
    "|refund_bucket                 \t|Varchar\t|Reason for refunding customer\n",
    "|refund_sub_bucket             \t|Varchar\t|Sub reason for refunding customer\n",
    "|transaction_date              \t|Date\t\t|date of refund\n",
    "|week_end_date                 \t|Date\t\t|weekend date of refund\n",
    "|dmm_subcat                    \t|Varchar\t|sub category of product\n",
    "|category                      \t|Varchar\t|category of product\n",
    "|deal_supply_channel           \t|Varchar\t|channel of sale\n",
    "|buyer_name                    \t|Varchar\t|name of buyer who sourced the product\n",
    "|auth_bookings                 \t|Float\t\t|bookings authorized on card\n",
    "|capture_bookings              \t|Float\t\t|bookings captured\n",
    "|refunds                       \t|Float\t\t|amount of refund\n",
    "|cancel_refunds                \t|Float\t\t|refunds if the transaction was a cancellation\n",
    "|refunded_units                \t|Integer\t|quantity of product for which refunds were issued\n",
    "|auth_refunds                  \t|Integer\t|\t\n",
    "|capture_units                 \t|Integer\t|\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow a faster processing and easy data manipulation let's subset our dataframe to separate one year of transactions and refunds based on a transaction date. I will take 2018 data. \n",
    "\n",
    "To do so I need to convert all date's columns to Python date-time format and will use a boolean mask to create a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_date'] = pd.to_datetime(df['order_date'], infer_datetime_format=True)\n",
    "df['transaction_date'] = pd.to_datetime(df['transaction_date'], infer_datetime_format=True)\n",
    "df['week_end_date'] = pd.to_datetime(df['week_end_date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I have already converted date's columns to date format, I can set a desired date to filter a dataframe. Then I will assign a mask to dateaframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2162199, 16)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = '2018-01-01'\n",
    "end_date = '2018-12-31'\n",
    "mask = (df['transaction_date'] >= start_date) & (df['transaction_date'] <= end_date)\n",
    "subset = df.loc[mask]\n",
    "subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = subset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['refunded_units'] = subset['refunded_units'].str.replace('?', '0').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we convereted refunded units into integer data type, let's work with auth_refunds, and capture_units in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['auth_refunds'] = subset['auth_refunds'].str.replace('?', '0').astype(int)\n",
    "subset['capture_units'] = subset['capture_units'].str.replace('?', '0').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert auth_bookings, capture_bookings, refunds, and cancel_refunds columns into the float data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['auth_bookings'] = subset['auth_bookings'].str.replace('?', '0').astype(float)\n",
    "subset['capture_bookings'] = subset['capture_bookings'].str.replace('?', '0').astype(float)\n",
    "subset['refunds'] = subset['refunds'].str.replace('?', '0').astype(float)\n",
    "subset['cancel_refunds'] = subset['cancel_refunds'].str.replace('?', '0').astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am done with data type transformations. Now let's replace question marks with unknown values in our categorical columns and see what categories and how many of them do I have.\n",
    "\n",
    "I have 139 unique subcategories in dummy subcategory column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "subset['dmm_subcat_1'] = subset['dmm_subcat_1'].str.replace('?', 'unknown')\n",
    "print(subset['dmm_subcat_1'].nunique())\n",
    "# print(subset['dmm_subcat_1'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have only 14 categories compare to 139 subcategories. In reality categories are actual product categories that Groupon sells online but for our project purposes they were replaced with college math classes names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "subset['category_1'] = subset['category_1'].str.replace('?', 'unknown')\n",
    "print(subset['category_1'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only two unique supply channels: goods or goods stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Goods           1935944\n",
       "Goods Stores     226255\n",
       "Name: deal_supply_channel, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset['deal_supply_channel'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buyer names are not the actual buyers but rather the employees who sell the particular products. There are 273 unique employees in our subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n"
     ]
    }
   ],
   "source": [
    "subset['buyer_name_1'] = subset['buyer_name_1'].str.replace('?', 'Unknown')\n",
    "print(subset['buyer_name_1'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our refund bucket column consists of seven buckets, which are general reasons for return.\n",
    "In addition, we have 18 subbuckets columns that also specify the refund reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['refund_bucket'] = subset['refund_bucket'].str.replace('?', 'Unknown')\n",
    "subset['refund_sub_bucket'] = subset['refund_sub_bucket'].str.replace('?', 'Unknown')\n",
    "print(subset['refund_sub_bucket'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Data Analysis and Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create year, month, and day columns to plot refund and transaction amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = pd.DatetimeIndex(subset['transaction_date'])\n",
    "subset['year'],subset['month'],subset['day'] = variable.year, variable.month, variable.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data = subset.groupby(['month','day']).agg({'refunds':np.mean,'capture_bookings':np.mean})\n",
    "avg_data.index = [datetime(2018,month,day) for (month, day) in avg_data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.figure(figsize=(18,6))\n",
    "\n",
    "# subplot 1\n",
    "plt.subplot(1,2,1)\n",
    "avg_data['refunds'].plot(colormap='seismic',alpha=0.4)\n",
    "plt.text('2018-12-25',62,'Christmas',ha='center')\n",
    "plt.text('2018-01-01',63.5,\"New Year's Day\",ha='center')\n",
    "plt.text('2018-02-14',50,\"Valentine's Day\",ha='center')\n",
    "plt.ylabel('Average refund amount, $')\n",
    "plt.title(r'Groupon Average Refund Amount')\n",
    "plt.margins(0.010, 0.025)\n",
    "\n",
    "# subplot 2\n",
    "plt.subplot(1,2,2)\n",
    "avg_data['capture_bookings'].plot(colormap='autumn', alpha=0.4)\n",
    "plt.text('2018-11-23',2675,'Thanksgiving',ha='center')\n",
    "plt.ylabel('Average transaction amount, $')\n",
    "plt.title(r'Groupon Average Transaction Amount')\n",
    "plt.margins(0.010, 0.025)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following plots, we can clearly see that refunds and transactions have spikes and valleys. A clear seasnal trend can be detected here. We can observe the biggest transaction spike during Thanksgiving, when most people shop for Christmas gifts. We can also observe the highest amount of refunds during Christmas, New Year's, and Valentine's Day, when people return gifts they didn't like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket, frame in subset.groupby('refund_bucket'):\n",
    "    avg = np.round(np.average(frame['refunds']),2)\n",
    "    print('The average amount of refund in ' + bucket + ' was ' + str(avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupon might not be able to decrease refunds in Other, Returns, or Two-Hour Refunds because of the nature of these refunds. However, the company can definetly decrease refunds in such categories as Logistic Cancellations, Shortage Cancellations, and Fraud because logistic and shortages are managed by the company.\n",
    "\n",
    "When we take a closer look at the average amounts of refunds by sub bucket, we see that the same rule applies to sub category column - there are definetly areas for improvement, especially in such sub buckets as Groupon error, Other Shortage, Shipping Issues, \n",
    "Warehouse Shortage, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.groupby(['refund_bucket','refund_sub_bucket']).agg(\n",
    "min_refund=pd.NamedAgg(column='refunds', aggfunc=np.min),\n",
    "min_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.min),\n",
    "max_refund=pd.NamedAgg(column='refunds', aggfunc=np.max),\n",
    "max_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.max),\n",
    "avg_refund=pd.NamedAgg(column='refunds', aggfunc=np.mean),\n",
    "avg_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping data by refund bucket gives us a very useful insight about the dataset: all valid transactions with no refunds fall into a category 'Unknown' under a refund bucket column, which makes the 'unknown' refund bucket the biggest refund category in the dataset. This fact makes data analysis harder because I don't actually know what products under each particular refund category were bought the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained by Groupon team, they do not use refund amount to analyze refunds. The better measure for this is a refund rate - refund amount compared to total transaction amount.\n",
    "\n",
    "Let's add a refund rate column to our dataset to generate some insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_category = subset.groupby(['category_1']).agg(\n",
    "min_refund=pd.NamedAgg(column='refunds', aggfunc=np.min),\n",
    "min_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.min),\n",
    "max_refund=pd.NamedAgg(column='refunds', aggfunc=np.max),\n",
    "max_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.max),\n",
    "avg_refund=pd.NamedAgg(column='refunds', aggfunc=np.mean),\n",
    "avg_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.mean)).sort_values(by = 'avg_transaction',ascending=False)\n",
    "by_category['refund_rate'] = (by_category['avg_refund']/by_category['avg_transaction'])*100\n",
    "by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "by_category.sort_values('refund_rate', inplace = True, ascending=False)\n",
    "x = by_category.index\n",
    "y = by_category['refund_rate']\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "barlist = plt.bar(x,y,color='grey',edgecolor='k',alpha=0.5)\n",
    "barlist[0].set_color('lawngreen')\n",
    "plt.title('Refund Rates by Product Category',fontsize=11)\n",
    "#plt.tight_layout()\n",
    "plt.margins(0,0)\n",
    "\n",
    "# rotate the tick labels for the x axis\n",
    "_ = plt.gca().xaxis\n",
    "for item in _.get_ticklabels():\n",
    "    item.set_rotation(45)\n",
    "    \n",
    "# remove all the ticks (both axes), and tick labels on the Y axis\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False,labelbottom=True)\n",
    "\n",
    "# remove the frame of the chart\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "    \n",
    "# direct label each bar with Y axis values\n",
    "for bar in barlist:\n",
    "    plt.gca().text(bar.get_x() + bar.get_width()/2, bar.get_height() - 2, str(int(bar.get_height())) + '%', \n",
    "                 ha='center', color='k', fontsize=11)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following graph, we can see that category Ensemble Methods has the highest refund rate, together with the lowest average amount of transactions and refunds as well. Operations research has second highest refund rate with the third lowest average amount of transactions and refunds as well. If these product categories are not that profitable and have the high refund rate, it is worth to take a closer look and make a decision if these products are worth selling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_subcategory = subset.groupby(['category_1','dmm_subcat_1']).agg(\n",
    "min_refund=pd.NamedAgg(column='refunds', aggfunc=np.min),\n",
    "min_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.min),\n",
    "max_refund=pd.NamedAgg(column='refunds', aggfunc=np.max),\n",
    "max_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.max),\n",
    "avg_refund=pd.NamedAgg(column='refunds', aggfunc=np.mean),\n",
    "avg_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.mean))\n",
    "by_subcategory['refund_rate'] = (by_subcategory['avg_refund']/by_subcategory['avg_transaction'])*100\n",
    "by_subcategory.sort_values(by=['avg_transaction','avg_refund','refund_rate'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest refund rate belongs to buyer Zoey but she is very productive employee with the highest amount of average transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_buyer = subset.groupby(['buyer_name_1']).agg(\n",
    "min_refund=pd.NamedAgg(column='refunds', aggfunc=np.min),\n",
    "min_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.min),\n",
    "max_refund=pd.NamedAgg(column='refunds', aggfunc=np.max),\n",
    "max_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.max),\n",
    "avg_refund=pd.NamedAgg(column='refunds', aggfunc=np.mean),\n",
    "avg_transaction=pd.NamedAgg(column='capture_bookings', aggfunc=np.mean))\n",
    "by_buyer['refund_rate'] = (by_buyer['avg_refund']/by_buyer['avg_transaction'])*100\n",
    "by_buyer.sort_values(by=['avg_transaction','avg_refund','refund_rate'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset.pivot_table(values='refunds', index='buyer_name_1', columns='refund_bucket', aggfunc = np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Machine Learning Modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = subset['refunds']\n",
    "X = subset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['order_date','week_end_date','auth_bookings','refunds','cancel_refunds','refunded_units','auth_refunds',         \n",
    "'capture_units','year','month','day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will need to use a customized label encoder as skikit learn label encoder only takes 1-d array as an argument. Please refer to [StackOverflow](https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# creating a customized label encoder as skikit learn label encoder only takes 1-d array as an argument\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "       Transforms columns of X specified in self.columns using\n",
    "       LabelEncoder(). If no columns specified, transforms all\n",
    "       columns in X.\n",
    "       '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_var = ['refund_bucket','refund_sub_bucket','dmm_subcat_1','category_1','deal_supply_channel',\n",
    "                   'buyer_name_1']\n",
    "X_train = MultiColumnLabelEncoder(columns = categorical_var).fit_transform(X_train)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = MultiColumnLabelEncoder(columns=categorical_var).transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['transaction_date'],inplace=True, axis=1)\n",
    "X_test.drop(['transaction_date'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=True).fit(X_train, y_train)\n",
    "predictions_reg = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.score(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
